import logging
from datetime import datetime
import concurrent.futures
import time

from app.utils.date import get_today_kst
from app.utils.dynamo import (
    save_frequency_summary,
    get_frequency_by_category_and_date,
    get_news_by_category_and_date,
    update_news_card_content,
)
from app.services.openai_service import summarize_articles, cluster_similar_texts, summarize_group
from app.services.tts_service import text_to_speech
from app.utils.s3 import upload_audio_to_s3_presigned
from app.constants.category_map import CATEGORY_MAP
from app.services.deepsearch_service import extract_content_flexibly

# ãƒ­ã‚°è¨­å®š
logger = logging.getLogger()
logger.setLevel(logging.INFO)

def process_single_category(category_ko: str, date: str) -> dict:
    """
    å˜ä¸€ã‚«ãƒ†ã‚´ãƒªå‡¦ç†é–¢æ•°ï¼ˆä¸¦åˆ—å‡¦ç†ç”¨ï¼‰
    """
    category_en = CATEGORY_MAP[category_ko]["api_name"]
    freq_id = f"{category_en}#{date}"
    start_time = time.time()
    
    try:
        # é‡è¤‡é˜²æ­¢ï¼šæ—¢ã«ç”Ÿæˆæ¸ˆã¿ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        if get_frequency_by_category_and_date(category_en, date):
            logger.info(f"ğŸš« ì´ë¯¸ ìƒì„±ë¨ â†’ ìŠ¤í‚µ: {freq_id}")
            return {"category": category_en, "status": "skipped", "reason": "already_exists"}

        logger.info(f"[{category_en}] ëŒ€ë³¸/ìŒì„± ìƒì„± ì‹œì‘")
        # è©²å½“ã‚«ãƒ†ã‚´ãƒªã®æœ¬æ—¥ã®è¨˜äº‹ã‚’å–å¾—
        articles = get_news_by_category_and_date(category_en, date)
        logger.info(f"[{category_en}] ìˆ˜ì§‘ëœ ê¸°ì‚¬ ìˆ˜: {len(articles)}")

        full_contents = []
        processed_count = 0
        target_count = 30  # æ­£ç¢ºã«30ä»¶ã«åˆ¶é™

        # è¨˜äº‹æœ¬æ–‡ã‚’æ­£ç¢ºã«30ä»¶ã¾ã§åé›†
        for i, article in enumerate(articles):
            if len(full_contents) >= target_count:
                logger.info(f"[{category_en}] ëª©í‘œ ë‹¬ì„±: {target_count}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
                break
                
            processed_count += 1
            news_id = article.get("news_id")
            url = article.get("content_url")
            content = article.get("content", "").strip()

            if not news_id or not url:
                logger.warning(f"[{category_en}] #{processed_count} URL ë˜ëŠ” ID ì—†ìŒ â†’ ìŠ¤í‚µ")
                continue

            # æœ¬æ–‡ãŒçŸ­ã„ã¾ãŸã¯å­˜åœ¨ã—ãªã„å ´åˆã¯å†æŠ½å‡ºã‚’è©¦ã¿ã‚‹
            if not content or len(content) < 300:
                try:
                    content = extract_content_flexibly(url)
                    if content and len(content) >= 300:
                        update_news_card_content(news_id, content)
                        logger.info(f"[{category_en}] #{processed_count} ë³¸ë¬¸ ë³´ì™„ ì €ì¥ ì™„ë£Œ ({len(content)}ì)")
                    else:
                        logger.warning(f"[{category_en}] #{processed_count} ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ë˜ëŠ” ë„ˆë¬´ ì§§ìŒ â†’ ìŠ¤í‚µ")
                        continue
                except Exception as e:
                    logger.warning(f"[{category_en}] #{processed_count} ë³¸ë¬¸ ì¬ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue

            # ãƒˆãƒ¼ã‚¯ãƒ³æœ€é©åŒ–ï¼š3000æ–‡å­—ã‹ã‚‰1500æ–‡å­—ã¸çŸ­ç¸®
            trimmed = content[:1500]
            full_contents.append(trimmed)
            logger.info(f"[{category_en}] #{len(full_contents)} ë³¸ë¬¸ ì‚¬ìš© ì™„ë£Œ ({len(trimmed)}ì)")

        logger.info(f"[{category_en}] ìµœì¢… ìˆ˜ì§‘ëœ ê¸°ì‚¬ ìˆ˜: {len(full_contents)}ê°œ (ëª©í‘œ: {target_count}ê°œ)")

        # æœ¬æ–‡æ•°ãŒå°‘ãªã™ãã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        if len(full_contents) < 5:
            logger.warning(f"[{category_en}] ìœ íš¨ ë³¸ë¬¸ ë¶€ì¡± ({len(full_contents)}ê°œ) â†’ ìŠ¤í‚µ")
            return {"category": category_en, "status": "failed", "reason": "insufficient_content"}

        # ä¸€æ¬¡ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼šé‡è¤‡è¨˜äº‹ã®é™¤å»
        logger.info(f"[{category_en}] 1ì°¨ í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘: {len(full_contents)}ê°œ ê¸°ì‚¬")
        try:
            groups = cluster_similar_texts(full_contents, threshold=0.80)
            group_summaries = []
            
            for group_idx, group in enumerate(groups):
                if len(group) == 1:
                    # å˜ä¸€è¨˜äº‹ã¯ãã®ã¾ã¾ä½¿ç”¨
                    group_summaries.append(group[0])
                    logger.info(f"[{category_en}] ê·¸ë£¹ #{group_idx+1}: ë‹¨ì¼ ê¸°ì‚¬ ({len(group[0])}ì)")
                else:
                    # è¤‡æ•°ã®é¡ä¼¼è¨˜äº‹ â†’ ä»£è¡¨è¦ç´„æ–‡ã‚’ç”Ÿæˆ
                    try:
                        summary = summarize_group(group, category_en)
                        group_summaries.append(summary)
                        logger.info(f"[{category_en}] ê·¸ë£¹ #{group_idx+1}: {len(group)}ê°œ ê¸°ì‚¬ â†’ í†µí•© ìš”ì•½ ({len(summary)}ì)")
                    except Exception as e:
                        logger.warning(f"[{category_en}] ê·¸ë£¹ #{group_idx+1} ìš”ì•½ ì‹¤íŒ¨, ì²« ë²ˆì§¸ ê¸°ì‚¬ ì‚¬ìš©: {e}")
                        group_summaries.append(group[0])
            
            logger.info(f"[{category_en}] 1ì°¨ í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ: {len(full_contents)}ê°œ â†’ {len(group_summaries)}ê°œ ê·¸ë£¹")
            final_contents = group_summaries
            
        except Exception as e:
            logger.warning(f"[{category_en}] 1ì°¨ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨, ì›ë³¸ ê¸°ì‚¬ ì‚¬ìš©: {e}")
            final_contents = full_contents

        # GPTã§æœ€çµ‚ã‚¹ã‚¯ãƒªãƒ—ãƒˆç”Ÿæˆ
        logger.info(f"[{category_en}] ëŒ€ë³¸ ìƒì„± ì‹œì‘: {len(final_contents)}ê°œ ê¸°ì‚¬")
        script = summarize_articles(final_contents, category_en)
        if not script or len(script) < 500:
            logger.warning(f"[{category_en}] ìš”ì•½ ê¸¸ì´ ë¶€ì¡± â†’ ìŠ¤í‚µ")
            return {"category": category_en, "status": "failed", "reason": "summary_too_short"}

        logger.info(f"[{category_en}] ìš”ì•½ ì™„ë£Œ: {len(script)}ì")

        # ElevenLabsã§TTSå¤‰æ›å¾Œã€S3ã®Presigned URLã‚’ç”Ÿæˆ
        try:
            audio_bytes = text_to_speech(script)
            audio_url = upload_audio_to_s3_presigned(
                file_bytes=audio_bytes,
                user_id="shared",
                category=category_en,
                date=date,
                expires_in_seconds=604800  # æœ‰åŠ¹æœŸé–“ï¼š7æ—¥é–“
            )
            logger.info(f"[{category_en}] TTS Presigned URL ìƒì„± ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"[{category_en}] TTS ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            return {"category": category_en, "status": "failed", "reason": f"tts_failed: {str(e)}"}

        # çµæœã‚’DynamoDBã«ä¿å­˜
        item = {
            "frequency_id": freq_id,
            "category": category_en,
            "date": date,
            "script": script,
            "audio_url": audio_url,
            "created_at": datetime.utcnow().isoformat()
        }

        save_frequency_summary(item)
        
        elapsed_time = time.time() - start_time
        logger.info(f"[{category_en}] DynamoDB ì €ì¥ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {elapsed_time:.1f}ì´ˆ)")
        
        return {
            "category": category_en, 
            "status": "success", 
            "script_length": len(script),
            "elapsed_time": elapsed_time
        }

    except Exception as e:
        elapsed_time = time.time() - start_time
        logger.exception(f"[{category_en}] ì²˜ë¦¬ ì‹¤íŒ¨ (ì†Œìš”ì‹œê°„: {elapsed_time:.1f}ì´ˆ): {str(e)}")
        return {
            "category": category_en, 
            "status": "failed", 
            "reason": f"exception: {str(e)}",
            "elapsed_time": elapsed_time
        }

def generate_all_frequencies():
    """
    æ¯æœ6æ™‚ã«è‡ªå‹•å®Ÿè¡Œï¼šã‚«ãƒ†ã‚´ãƒªåˆ¥ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹æœ¬æ–‡ã‚’ã‚‚ã¨ã«å…±é€šå°æœ¬ãƒ»éŸ³å£°ã‚’ç”Ÿæˆï¼ˆä¸¦åˆ—å‡¦ç†ï¼‰
    - ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚«ãƒ¼ãƒ‰DBã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªã”ã¨ã«æ­£ç¢ºã«30ä»¶ã®è¨˜äº‹ã‚’ä½¿ç”¨ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³æœ€é©åŒ–ï¼‰
    - ä¸è¶³ã—ã¦ã„ã‚‹æœ¬æ–‡ã¯å†æŠ½å‡º
    - GPTã§è¦ç´„ã—ã€ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ç”Ÿæˆï¼ˆã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å«ã‚€ï¼‰
    - ElevenLabsã®TTSã§éŸ³å£°å¤‰æ›ã—ã€S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    - Frequenciesãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¿å­˜ï¼ˆã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼‹Presigned MP3ãƒªãƒ³ã‚¯ï¼‰
    """
    date = get_today_kst()
    all_categories = list(CATEGORY_MAP.keys())
    total_start_time = time.time()
    
    logger.info(f"ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘: {len(all_categories)}ê°œ ì¹´í…Œê³ ë¦¬ ë™ì‹œ ì²˜ë¦¬")
    # ä¸¦åˆ—å‡¦ç†é–‹å§‹ï¼š{len(all_categories)}ä»¶ã®ã‚«ãƒ†ã‚´ãƒªã‚’åŒæ™‚å‡¦ç†
    logger.info(f"ì¹´í…Œê³ ë¦¬ ëª©ë¡: {all_categories}")
    # ã‚«ãƒ†ã‚´ãƒªä¸€è¦§ã®å‡ºåŠ›

    # ä¸¦åˆ—å‡¦ç†ï¼šThreadPoolExecutorã‚’ä½¿ç”¨
    results = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=6) as executor:  # 6ã‚«ãƒ†ã‚´ãƒªã‚’åŒæ™‚ã«å‡¦ç†
        # å„ã‚«ãƒ†ã‚´ãƒªã‚’ä¸¦åˆ—ã§å‡¦ç†ã™ã‚‹Futureã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ç”Ÿæˆ
        future_to_category = {
            executor.submit(process_single_category, category_ko, date): category_ko 
            for category_ko in all_categories
        }
        
        # å®Œäº†ã—ãŸé †ã«çµæœã‚’åé›†
        for future in concurrent.futures.as_completed(future_to_category):
            category_ko = future_to_category[future]
            try:
                result = future.result()
                results.append(result)
                
                if result["status"] == "success":
                    logger.info(f"[{result['category']}] ì„±ê³µ ì™„ë£Œ - ëŒ€ë³¸: {result['script_length']}ì, ì†Œìš”ì‹œê°„: {result['elapsed_time']:.1f}ì´ˆ")
                    # æˆåŠŸå®Œäº† - å°æœ¬ï¼š{æ–‡å­—æ•°}æ–‡å­—ã€æ‰€è¦æ™‚é–“ï¼š{ç§’æ•°}ç§’
                elif result["status"] == "skipped":
                    logger.info(f"[{result['category']}] ìŠ¤í‚µë¨ - ì‚¬ìœ : {result['reason']}")
                    # ã‚¹ã‚­ãƒƒãƒ— - ç†ç”±ï¼š{reason}
                else:
                    logger.warning(f"[{result['category']}] ì‹¤íŒ¨ - ì‚¬ìœ : {result['reason']}")
                    # å¤±æ•— - ç†ç”±ï¼š{reason}
                    
            except Exception as exc:
                logger.exception(f"[{category_ko}] ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {exc}")
                # æƒ³å®šå¤–ã®ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ
                results.append({
                    "category": CATEGORY_MAP[category_ko]["api_name"], 
                    "status": "failed", 
                    "reason": f"executor_exception: {str(exc)}"
                })

    # å…¨ä½“ã®çµæœã‚’è¦ç´„
    total_elapsed_time = time.time() - total_start_time
    success_count = sum(1 for r in results if r["status"] == "success")
    failed_count = sum(1 for r in results if r["status"] == "failed")
    skipped_count = sum(1 for r in results if r["status"] == "skipped")
    
    logger.info("ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ!")
    # ä¸¦åˆ—å‡¦ç†å®Œäº†ï¼
    logger.info(f"ì´ ì†Œìš”ì‹œê°„: {total_elapsed_time:.1f}ì´ˆ")
    # ç·æ‰€è¦æ™‚é–“
    logger.info(f"ê²°ê³¼ ìš”ì•½: ì„±ê³µ {success_count}ê°œ, ì‹¤íŒ¨ {failed_count}ê°œ, ìŠ¤í‚µ {skipped_count}ê°œ")
    # æˆåŠŸä»¶æ•°ã€å¤±æ•—ä»¶æ•°ã€ã‚¹ã‚­ãƒƒãƒ—ä»¶æ•°ã®è¦ç´„
    
    # å„ã‚«ãƒ†ã‚´ãƒªã”ã¨ã®è©³ç´°çµæœãƒ­ã‚°
    for result in results:
        status_text = {"success": "SUCCESS", "failed": "FAILED", "skipped": "SKIPPED"}.get(result["status"], "UNKNOWN")
        logger.info(f"{status_text} {result['category']}: {result['status']}")
        if "reason" in result:
            logger.info(f"   â””â”€ ì‚¬ìœ : {result['reason']}")
            # ç†ç”±ã®å‡ºåŠ›

    return results
