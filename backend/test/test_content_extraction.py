#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# test/test_content_extraction.py

import os
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
from dotenv import load_dotenv
load_dotenv(os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), '.env'))

from app.services.deepsearch_service import (
    is_korean_text, 
    clean_text_noise,
    extract_content_with_bs4,
    extract_content_flexibly
)

def test_korean_text_detection():
    """í•œê¸€ í…ìŠ¤íŠ¸ ê°ì§€ í…ŒìŠ¤íŠ¸"""
    print(" [í…ŒìŠ¤íŠ¸ 1] í•œê¸€ í…ìŠ¤íŠ¸ ê°ì§€")
    
    test_cases = [
        ("ì•ˆë…•í•˜ì„¸ìš”. í•œêµ­ì–´ ë‰´ìŠ¤ ê¸°ì‚¬ì…ë‹ˆë‹¤.", True, "ìˆœìˆ˜ í•œê¸€"),
        ("Hello, this is English text.", False, "ì˜ì–´ í…ìŠ¤íŠ¸"),
        ("ì•ˆë…•í•˜ì„¸ìš” hello world ë°˜ê°‘ìŠµë‹ˆë‹¤.", True, "í•œì˜ í˜¼í•© (í•œê¸€ ìš°ì„¸)"),
        ("Hello ì•ˆë…• world test ë°˜ê°€ì›Œ", False, "í•œì˜ í˜¼í•© (ì˜ì–´ ìš°ì„¸)"),
        ("123456789", False, "ìˆ«ìë§Œ"),
        ("", False, "ë¹ˆ ë¬¸ìì—´"),
        ("ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ì˜ ë‰´ìŠ¤ë¥¼ ì „í•´ë“œë¦½ë‹ˆë‹¤. ì •ì¹˜, ê²½ì œ, ì‚¬íšŒ ë¶„ì•¼ì˜ ì†Œì‹ì´ ìˆìŠµë‹ˆë‹¤.", True, "ê¸´ í•œê¸€ í…ìŠ¤íŠ¸")
    ]
    
    for text, expected, description in test_cases:
        result = is_korean_text(text, threshold=0.7)
        status = "" if result == expected else ""
        print(f"  {status} {description}: {result} (ì˜ˆìƒ: {expected})")
        if text:
            korean_ratio = len([c for c in text if 'ê°€' <= c <= 'í£']) / len([c for c in text if c.isalpha() or 'ê°€' <= c <= 'í£']) if any(c.isalpha() or 'ê°€' <= c <= 'í£' for c in text) else 0
            print(f"    â†’ í•œê¸€ ë¹„ìœ¨: {korean_ratio:.2f}")
    print()

def test_text_noise_cleaning():
    """í…ìŠ¤íŠ¸ ë…¸ì´ì¦ˆ ì œê±° í…ŒìŠ¤íŠ¸"""
    print(" [í…ŒìŠ¤íŠ¸ 2] í…ìŠ¤íŠ¸ ë…¸ì´ì¦ˆ ì œê±°")
    
    noisy_text = """
ì´ê²ƒì€ ì‹¤ì œ ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ì…ë‹ˆë‹¤.
ì¤‘ìš”í•œ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

ê¹€ê¸°ì reporter@news.com
í™ê¸¸ë™ê¸°ì
ì „í™”: 02-1234-5678

[ì¹´ì¹´ì˜¤í†¡] @newschannel
[ë©”ì¼] contact@example.com

Copyright 2024 News Corp. All rights reserved.
ë¬´ë‹¨ì „ì¬ ë° ì¬ë°°í¬ ê¸ˆì§€
ì¬ë°°í¬ ê¸ˆì§€

ì´ ê¸°ì‚¬ì˜ ëŒ“ê¸€ ì •ì±…ì„ ê²°ì •í•©ë‹ˆë‹¤
ì•± ë‹¤ìš´ë¡œë“œ ë§í¬
ë„¤ì´ë²„ AI ë‰´ìŠ¤ ì•Œê³ ë¦¬ì¦˜

ì‹¤ì œ ë‰´ìŠ¤ ë‚´ìš©ì´ ì—¬ê¸°ì— ìˆìŠµë‹ˆë‹¤.
ë¶„ì„ê³¼ ì˜ê²¬ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
"""
    
    cleaned = clean_text_noise(noisy_text)
    
    print("ì›ë³¸ í…ìŠ¤íŠ¸ ì¤„ ìˆ˜:", len(noisy_text.strip().split('\n')))
    print("ì •ì œ í›„ ì¤„ ìˆ˜:", len(cleaned.strip().split('\n')))
    print("\nì •ì œëœ í…ìŠ¤íŠ¸:")
    print(cleaned)
    
    # ë…¸ì´ì¦ˆ ì œê±° í™•ì¸
    noise_keywords = ["ê¸°ì", "@", "Copyright", "ë¬´ë‹¨ì „ì¬", "ì¹´ì¹´ì˜¤í†¡", "ëŒ“ê¸€ ì •ì±…", "ì•± ë‹¤ìš´"]
    remaining_noise = [kw for kw in noise_keywords if kw in cleaned]
    
    if remaining_noise:
        print(f" ë‚¨ì€ ë…¸ì´ì¦ˆ: {remaining_noise}")
    else:
        print(" ë…¸ì´ì¦ˆ ì œê±° ì™„ë£Œ")
    print()

def test_content_extraction_simulation():
    """ë³¸ë¬¸ ì¶”ì¶œ ì‹œë®¬ë ˆì´ì…˜ í…ŒìŠ¤íŠ¸"""
    print(" [í…ŒìŠ¤íŠ¸ 3] ë³¸ë¬¸ ì¶”ì¶œ ì‹œë®¬ë ˆì´ì…˜")
    
    # ì‹¤ì œ URL í…ŒìŠ¤íŠ¸ëŠ” ì™¸ë¶€ ì˜ì¡´ì„±ì´ ìˆìœ¼ë¯€ë¡œ ì‹œë®¬ë ˆì´ì…˜
    test_urls = [
        "https://newsis.com/article/123",
        "https://yna.co.kr/article/456", 
        "https://kbs.co.kr/news/789",
        "https://unknown-site.com/article/999"
    ]
    
    print(" í…ŒìŠ¤íŠ¸ URL ëª©ë¡:")
    for i, url in enumerate(test_urls, 1):
        domain = url.split('/')[2].replace('www.', '')
        print(f"  {i}. {domain} ({url})")
    
    print(f"\n selector ì§€ì› ë„ë©”ì¸ í™•ì¸:")
    from app.services.deepsearch_service import ARTICLE_SELECTORS
    
    supported_count = 0
    for url in test_urls:
        domain = url.split('/')[2].replace('www.', '')
        has_selector = domain in ARTICLE_SELECTORS
        status = "" if has_selector else ""
        print(f"  {status} {domain}: {'ì§€ì›ë¨' if has_selector else 'ì¼ë°˜ ì¶”ì¶œ'}")
        if has_selector:
            supported_count += 1
    
    print(f"\n selector ì§€ì›ë¥ : {supported_count}/{len(test_urls)} ({supported_count/len(test_urls)*100:.0f}%)")
    print()

def test_article_selectors():
    """ê¸°ì‚¬ selector ì •ì˜ í…ŒìŠ¤íŠ¸"""
    print(" [í…ŒìŠ¤íŠ¸ 4] ê¸°ì‚¬ selector ì •ì˜")
    
    from app.services.deepsearch_service import ARTICLE_SELECTORS
    
    print(f" ì´ ì§€ì› ë„ë©”ì¸: {len(ARTICLE_SELECTORS)}ê°œ")
    print("\nì§€ì› ë„ë©”ì¸ ëª©ë¡:")
    
    major_domains = ["newsis.com", "yna.co.kr", "kbs.co.kr", "donga.com", "joongang.co.kr"]
    
    for i, (domain, selector) in enumerate(ARTICLE_SELECTORS.items(), 1):
        is_major = "â­" if domain in major_domains else "  "
        print(f"{is_major} {i:2d}. {domain:<20} â†’ {selector}")
    
    print(f"\n ì£¼ìš” ì–¸ë¡ ì‚¬ ì§€ì›: {sum(1 for d in major_domains if d in ARTICLE_SELECTORS)}/{len(major_domains)}ê°œ")
    print()

def test_unwanted_keywords():
    """ë¶ˆí•„ìš” í‚¤ì›Œë“œ íŒ¨í„´ í…ŒìŠ¤íŠ¸"""
    print(" [í…ŒìŠ¤íŠ¸ 5] ë¶ˆí•„ìš” í‚¤ì›Œë“œ íŒ¨í„´")
    
    from app.services.deepsearch_service import UNWANTED_KEYWORDS
    
    test_sentences = [
        "ì´ ê¸°ì‚¬ì˜ ëŒ“ê¸€ ì •ì±…ì„ ê²°ì •í•©ë‹ˆë‹¤",
        "ì•± ë‹¤ìš´ë¡œë“œë¥¼ ê¶Œí•©ë‹ˆë‹¤", 
        "ë„¤ì´ë²„ AI ë‰´ìŠ¤ ì•Œê³ ë¦¬ì¦˜ì´ ì¶”ì²œí•œ ê¸°ì‚¬",
        "ì‹¤ì œ ë‰´ìŠ¤ ë‚´ìš©ì…ë‹ˆë‹¤",
        "í”„ë¦¬ë¯¸ì—„ì½˜í…ì¸  êµ¬ë… ì•ˆë‚´",
        "ì •ì¹˜ ë¶„ì•¼ì˜ ì¤‘ìš”í•œ ì†Œì‹ì…ë‹ˆë‹¤"
    ]
    
    print(f" ì´ ë¶ˆí•„ìš” í‚¤ì›Œë“œ: {len(UNWANTED_KEYWORDS)}ê°œ")
    print("\ní…ŒìŠ¤íŠ¸ ë¬¸ì¥ í•„í„°ë§:")
    
    for sentence in test_sentences:
        is_unwanted = any(kw in sentence for kw in UNWANTED_KEYWORDS)
        status = "ğŸš«" if is_unwanted else ""
        action = "ì œê±°ë¨" if is_unwanted else "ìœ ì§€ë¨"
        print(f"  {status} {sentence} â†’ {action}")
    
    print()

def main():
    """ë³¸ë¬¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print(" ë³¸ë¬¸ ì¶”ì¶œ ì„œë¹„ìŠ¤ í…ŒìŠ¤íŠ¸ ì‹œì‘\n")
    
    test_korean_text_detection()
    test_text_noise_cleaning()
    test_content_extraction_simulation()
    test_article_selectors()
    test_unwanted_keywords()
    
    print(" í…ŒìŠ¤íŠ¸ ìš”ì•½:")
    print(" í•œê¸€ í…ìŠ¤íŠ¸ ê°ì§€: ë‹¤ì–‘í•œ ì¼€ì´ìŠ¤ ê²€ì¦")
    print(" ë…¸ì´ì¦ˆ ì œê±°: ê¸°ìì •ë³´, ì €ì‘ê¶Œ, ê´‘ê³  í…ìŠ¤íŠ¸ í•„í„°ë§")
    print(" Selector ì§€ì›: ì£¼ìš” ì–¸ë¡ ì‚¬ ë„ë©”ì¸ ëŒ€ì‘")
    print(" í‚¤ì›Œë“œ í•„í„°: ë¶ˆí•„ìš”í•œ ì•ˆë‚´ë¬¸êµ¬ ì œê±°")
    
    print("\n ë³¸ë¬¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

if __name__ == "__main__":
    main() 